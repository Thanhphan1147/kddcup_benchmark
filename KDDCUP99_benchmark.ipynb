{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing anomaly detection algorithms on KDDCUP99\n",
    "based on https://github.com/elena-sharova/IsolationForest/blob/master/IsolationForest_v0.1.ipynb\n",
    "## KDDCUP 99\n",
    "\n",
    "The 1998 DARPA Intrusion Detection Evaluation Program was prepared and managed by MIT Lincoln Labs. The objective was to survey and evaluate research in intrusion detection.  A standard set of data to be audited, which includes a wide variety of intrusions simulated in a military network environment, was provided.  The 1999 KDD intrusion detection contest uses a version of this dataset.\n",
    "\n",
    "`SA` is obtained by simply selecting all the normal data, and a small proportion of abnormal data to give an anomaly ratio of 1%. SA has all 41 attributes.\n",
    "\n",
    "`SF` is the data where attribute logged_in is positive, thus focusing on the intrusion attack, which gives an anomaly ratio 0.3%. SF has log-transformed 4 attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.20.2\n",
      "pandas 1.2.4\n"
     ]
    }
   ],
   "source": [
    "print(\"numpy:\",np.__version__)\n",
    "print(\"pandas\", pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDDCUP SF dataset\n",
    "This dataset contains 41 attributes, has 703,067 records and an anomaly rate of 0.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"target\"\n",
    "service = \"service\"\n",
    "sf = datasets.fetch_kddcup99(subset='SF', percent10=True)\n",
    "# dfSF=pd.DataFrame(sf.data, columns=[\"duration\", \"service\", \"src_bytes\", \"dst_bytes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SF anomaly rate is 4.5%'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsf = pd.DataFrame(sf.data, columns=[\"duration\", \"service\", \"src_bytes\", \"dst_bytes\"])\n",
    "dfsf[target]=sf.target\n",
    "\n",
    "anomaly_rate_sf = 1.0 - len(dfsf.loc[dfsf[target]==b'normal.'])/len(dfsf)\n",
    "f\"SF anomaly rate is {anomaly_rate_sf:.1%}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byte_decoder(val):\n",
    "    # decodes byte literals to strings\n",
    "    \n",
    "    return val.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing for sf\n",
    "non-numeric attributes are label encoded into integers. all targets are converted to 1 (b'normal.') or -1\n",
    "For the sf subset, 2 out of 4 columns needs to be label encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration  service src_bytes dst_bytes      target\n",
      "0 -2.302585  b'http'  5.199049  8.603389  b'normal.'\n",
      "list of sf service values : {b'auth', b'pop_3', b'ftp_data', b'other', b'smtp', b'login', b'telnet', b'http', b'ftp', b'gopher', b'nntp', b'domain', b'private', b'discard', b'ssh', b'imap4', b'X11', b'IRC'}\n",
      "list of sa service values : {b'imap.', b'perl.', b'multihop.', b'spy.', b'normal.', b'satan.', b'buffer_overflow.', b'guess_passwd.', b'rootkit.', b'ftp_write.', b'warezmaster.', b'warezclient.', b'back.', b'ipsweep.', b'loadmodule.', b'phf.'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{dfsf.head(1)}\")\n",
    "print(f\"list of sf service values : {set(dfsf[service])}\")\n",
    "print(f\"list of sa service values : {set(dfsf[target])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "toDecodeSF = [service, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply hot encoding to fields of type string\n",
    "# convert all abnormal target types to single anomaly class\n",
    "\n",
    "dfsf['binary_target'] = [1 if x==b'normal.' else -1 for x in dfsf[target]]\n",
    "    \n",
    "leSF = preprocessing.LabelEncoder()\n",
    "\n",
    "for f in toDecodeSF:\n",
    "    dfsf[f] = list(map(byte_decoder, dfsf[f]))\n",
    "    dfsf[f] = leSF.fit_transform(dfsf[f])\n",
    "\n",
    "dfsf_normed = preprocessing.normalize(dfsf.drop([target, 'binary_target'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation forest on 10% SF\n",
    "First we split the dataset into a train and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sf, X_test_sf, y_train_sf, y_test_sf = train_test_split(dfsf.drop([target, 'binary_target'], axis=1), \n",
    "                                                                dfsf['binary_target'], test_size=0.33, random_state=11)\n",
    "X_train_nd, X_test_nd, y_train_nd, y_test_nd = train_test_split(dfsf_normed, dfsf['binary_target'], \n",
    "                                                    test_size=0.33, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "* num_estimators = 100\n",
    "* max_samples = 25%\n",
    "* contamination = 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifsf = IsolationForest(max_samples=0.25, random_state=11, contamination = 0.15, n_estimators=100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning Isolation Forest on the SF dataset.10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainning finished in : 0:00:04.252033'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "ifsf.fit(X_train_sf,y_train_sf)\n",
    "y_pred_train = ifsf.predict(X_train_sf)\n",
    "end = datetime.datetime.now()\n",
    "f\"trainning finished in : {end-start}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lofsf = LocalOutlierFactor(n_neighbors=15, metric='euclidean', algorithm = 'auto', contamination=0.15, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning Local Outlier Factor on the SF dataset.10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainning finished in : 0:00:00.312001'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "y_pred_train_lof = lofsf.fit_predict(X_train_nd,y_train_nd)\n",
    "end = datetime.datetime.now()\n",
    "f\"trainning finished in : {end-start}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IF results on SF.10% trainning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anomaly       0.11      0.37      0.17      2225\n",
      "      normal       0.97      0.86      0.91     46843\n",
      "\n",
      "    accuracy                           0.84     49068\n",
      "   macro avg       0.54      0.61      0.54     49068\n",
      "weighted avg       0.93      0.84      0.88     49068\n",
      "\n",
      "AUC:  61.4%\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_sf, y_pred_train, target_names=['anomaly', 'normal']))\n",
    "print (\"AUC: \", \"{:.1%}\".format(roc_auc_score(y_train_sf, y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOF results on SF.10% trainning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anomaly       0.03      0.09      0.04      2225\n",
      "      normal       0.95      0.85      0.90     46843\n",
      "\n",
      "    accuracy                           0.81     49068\n",
      "   macro avg       0.49      0.47      0.47     49068\n",
      "weighted avg       0.91      0.81      0.86     49068\n",
      "\n",
      "AUC:  46.7%\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_nd, y_pred_train_lof, target_names=['anomaly', 'normal']))\n",
    "print (\"AUC: \", \"{:.1%}\".format(roc_auc_score(y_train_nd, y_pred_train_lof)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SA anomaly rate is 3.4%'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = datasets.fetch_kddcup99(subset='SA', percent10=True)\n",
    "dfsa=pd.DataFrame(sa.data, \n",
    "                  columns=[\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\n",
    "                           \"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\n",
    "                           \"num_root\",\"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\n",
    "                           \"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\n",
    "                           \"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "                           \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "                           \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "                           \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"])\n",
    "assert len(dfsa)>0, \"SA dataset not loaded.\"\n",
    "\n",
    "dfsa[target]=sa.target\n",
    "\n",
    "anomaly_rate_sa = 1.0 - len(dfsa.loc[dfsa[target]==b'normal.'])/len(dfsa)\n",
    "f\"SA anomaly rate is {anomaly_rate_sa:.1%}\"\n",
    "\n",
    "# dfsa[\"target\"].head(5)\n",
    "# set(dfsa[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTP\n",
    "Using the 'service' attribute, the data is divided into {http, smtp, ftp, ftp_data, others} subsets. Here, only 'http' service data is used. Since the continuous attribute values are concentrated around '0', we transformed each value into a value far from '0', by y = log(x + 0.1). The original data set has 3,925,651 attacks (80.1%) out of 4,898,431 records. A smaller set is forged by having only 3,377 attacks (0.35%) of 976,157 records, where attribute 'logged_in' is positive. From this forged dataset 567,497 ‘http’ service data is used to construct the http (KDDCUP99) dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = datasets.fetch_kddcup99(subset='http', percent10=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kddcup",
   "language": "python",
   "name": "kddcup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
